{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Notebook\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task is to produce a neural process which is tailored to solve a physical system.  In this case, the physical system to be solved is the heat diffusion across a two-dimensional grid.  There are three main changes I wish to make to the standard neural process.  These are listed below, along with a brief description of their physical motivation.\n",
    "\n",
    "#### 1. Convolutional Encoder and Decoder block \n",
    "Spatial equivariance is built into the system, since the heat diffusion function does not have a spatially dependent forcing term.  All terms are based on derivatives, which means the solution obeys spatial equivariance.  I hope to replicate this feature in the architecture, allowing for an easier learning process.\n",
    "#### 2. Learning using Privileged Information\n",
    "Learning using privileged information (LUPI) allows the model, while training, to have access to 'privileged information' (PI).  PI could be anything important the model should be aware of.  For example, if the total energy in the grid is conserved, imput this as PI.  When it comes to test time, the model does not have access to any PI.  The hope is that the model can infer what the PI should be (e.g. by figuring out what the total energy is and conserving it), making for a faster learning process.  There is a wide variety of PI to be used and experimented with.  A few more examples are: initial conditions, final conditions, decay constant, entropy measure.\n",
    "\n",
    "The way the PI is implemented is through an encoder which runs separately to the usual encoder.  The PI embedding is then aggregated with the embedding of the data using a residual neural network; we do not want the PI to dominate the learning, but merely to add a `first order correction' to the learning process.  The good thing about this structure is that the PI can be added to any encoder block (e.g. flat encoder or convolutional encoder)\n",
    "\n",
    "#### 3. Time-Order Respecting Aggregator\n",
    "In a neural process, the embeddings of the data at different times, $t_i$, are aggregated with a permutation invariant aggregator.  This is not usually useful for physical processes, as there tends to be a time-respecting process such as increase in entropy (in this case, diffusion of heat).  Therefore, it might be useful to reflect this in the aggregator.  The aggregator is such that there is no information leak from future to past - it imposes causality.\n",
    "\n",
    "The aggregator could also be designed to reflect temporal equivariance, for the same reasons as the spatial equivariance: the diffusion equation involves no terms which depend explicitly on time, but rather only the derivative with resepct to time.  If the model is temporally equivariant, it may perform much better on extrapolation tasks (as it is not completely `blind' when it heads out of the sample, having learnt temporal behaviour)\n",
    "\n",
    "One possible choice for this is to allow the model to `learn' these features by implementing a multi-head attention network.  However, with this option there is far less control over which physical concepts I am imposing.\n",
    "\n",
    "\n",
    "I hope these three changes will improve on the most basic vanilla neural process architecture to learn the heat diffusion behaviour over a two-dimensional grid, as they are all grounded in physical concepts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "\n",
    "# grab the neural process functions from Emilien Dupont's library called neural-processes\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"./neural-processes\")\n",
    "\n",
    "# neural_process.py and training.py exist in neural-processes folder\n",
    "import neural_process\n",
    "from neural_process import NeuralProcessConv, NeuralProcess\n",
    "from training import NeuralProcessTrainer\n",
    "from utils import context_target_split\n",
    "from heat_diffusion_dataset import Initial_Conditions, Diffusion_Data, RestoredData\n",
    "from visualise import *\n",
    "\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams.update(\n",
    "    {\n",
    "        \"font.family\": \"sans-serif\",\n",
    "        \"font.size\": 12,\n",
    "        \"figure.figsize\": [8, 6],\n",
    "        \"lines.linewidth\": 1,\n",
    "        \"lines.markersize\": 8,\n",
    "        \"legend.fontsize\": \"medium\",\n",
    "        \"axes.labelsize\": \"medium\",\n",
    "        \"xtick.labelsize\": \"medium\",\n",
    "        \"ytick.labelsize\": \"medium\",\n",
    "    }\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion Data\n",
    "\n",
    "Heat Diffusion data was chosen as a suitable dataset to test my model.  The equation which the data follows is \n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial u(x, y, t)}{\\partial t} = D \\nabla^2 u(x, y, t)\n",
    "\\end{equation} \n",
    "\n",
    "The boundary conditions are chosen to be Neumann Boundary Conditions, specifying the normal gradient to be zero at the boundaries\n",
    "\n",
    "\\begin{equation}\n",
    "\\left. \\frac{\\partial u}{\\partial x} \\right|_{x = 0, L} = \\left. \\frac{\\partial u}{\\partial y}\\right|_{y=0, L} = 0\n",
    "\\end{equation}\n",
    "\n",
    "These boundary conditions ensure heat remains inside the grid and cannot escape (the flux through the boundary is zero).  There are no sources or sinks, so the total heat energy in the grid is conserved.\n",
    "\n",
    "The variation between datasets lies in the initial conditions.  There are three parameters for the initial conditions which are changed. \n",
    "1. Initial temperature \n",
    "2. Location of initial temperature spike\n",
    "3. Broadness of initial temperature spike \n",
    "\n",
    "The overall heat energy in the grid therefore can change from dataset to dataset, but remains constant within each one.  The diffusion coefficient is also the same for each dataset.  This could replicate, for example, heat diffusion in a fixed medium with varying initial conditions.\n",
    "\n",
    "Finite difference method (FDM) is used to solve equation 1.  The Laplacian is calculated using the five-point stencil finite difference method *Appendix.  This posed a difficulty when the initial conditions contained a discontinuous jump in temperature because Laplacian would become very large near the jump points.  To solve this issue, instead of implementing a different more advanced solver such as finite element methods (which may take a longer time to compute), a Gaussian filter was applied to smooth the initial conditions *Appendix.  The Gaussian filter was imported from `scipy.ndimage` module.\n",
    "\n",
    "FDM becomes unstable when $\\Delta t \\leq \\frac{\\Delta x^2}{4D}$ *appendix, reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = {\n",
    "    't_dim' : 1,\n",
    "    'y_dim' : 2500,\n",
    "    'r_dim' : 128,\n",
    "    'h_dim' : 128,\n",
    "    'z_dim' : 128,\n",
    "    'max_iter_time' : 500,\n",
    "    'grid_size' : 50,\n",
    "    'num_samples' : 40,\n",
    "    'num_context' : 30,\n",
    "    'num_target' : 30,\n",
    "    'batch_size' : 2,\n",
    "    'num_channels' : 16,\n",
    "    'pi_dim' : 4\n",
    "}\n",
    "\n",
    "assert dimensions['grid_size']**2 == dimensions['y_dim']\n",
    "assert dimensions['num_context'] + dimensions['num_target'] <= dimensions['max_iter_time']\n",
    "\n",
    "# parameters\n",
    "grid_size = dimensions['grid_size']\n",
    "num_samples = dimensions['num_samples']\n",
    "square_range = (5, 15) # side length of initial hot square\n",
    "temp_range = (1.0, 2.0)\n",
    "diffusion_coef = 0.25 * dimensions['max_iter_time'] # upper limit for stable FDM solver\n",
    "\n",
    "assert diffusion_coef <= 0.25 * dimensions['max_iter_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "RUN TO RE-GENERATE DATA\n",
    "'''\n",
    "\n",
    "initial_conditions = Initial_Conditions(\n",
    "    max_iter_time=dimensions['max_iter_time'], grid_size=grid_size\n",
    ")\n",
    "\n",
    "meta_data = Diffusion_Data(\n",
    "    num_samples=num_samples,\n",
    "    max_iter_time=dimensions['max_iter_time'],\n",
    "    grid_size=grid_size,\n",
    "    initial_conditions=initial_conditions,\n",
    "    square_range=square_range,\n",
    "    temp_range=temp_range,\n",
    "    diffusion_coef=diffusion_coef,\n",
    "    sigma=3\n",
    ")\n",
    "\n",
    "time_array = meta_data.time_array\n",
    "meta_data.visualise_solution(show_until=1, i=0)\n",
    "\n",
    "# meta_data.save_tensors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FLAT NEURAL PROCESS TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuralprocess = NeuralProcess(dimensions)\n",
    "\n",
    "data_loader = DataLoader(meta_data, batch_size=dimensions['batch_size'], shuffle=True)\n",
    "optimizer = torch.optim.Adam(neuralprocess.parameters(), lr=3e-4)\n",
    "np_trainer = NeuralProcessTrainer(\n",
    "    \"cuda\",\n",
    "    neuralprocess,\n",
    "    optimizer,\n",
    "    num_context_range=(dimensions['num_context'], dimensions['num_context']),\n",
    "    num_extra_target_range=(dimensions['num_target'], dimensions['num_target']),\n",
    "    print_freq=200,\n",
    "    is_lupi=False\n",
    ")\n",
    "\n",
    "neuralprocess.training = True\n",
    "np_trainer.train(data_loader, 2)\n",
    "\n",
    "torch.save(neuralprocess.state_dict(), './trainers/neuralprocess_dict')\n",
    "torch.save(np_trainer.epoch_loss_history, './metadata/epoch_loss_history')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONVOLUTIONAL NEURAL PROCESS TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters\n",
    "- h_dim\n",
    "- r_dim\n",
    "- z_dim\n",
    "- batch_size\n",
    "- num_context\n",
    "- num_target\n",
    "- lr\n",
    "\n",
    "#### Hidden Hyperparameters\n",
    "- Number of linear layers after convolution in Net\n",
    "- Number of linear layers in TransposeConvNet\n",
    "- Number of channels in Net and TransposeConvNet\n",
    "- Number of layers in Net and TransposeConvNet\n",
    "- Kernels, Dilations, Paddings\n",
    "\n",
    "#### Current Convolution Summary \n",
    "\n",
    "Net: 2 layers with 32 'hidden' channels, followed by 3 Linear layers with h_dim hidden.\n",
    "\n",
    "TNet: 1 Linear layer, followed by 2 transpose convolutional layers with 32 'hidden' channels.\n",
    "\n",
    "\n",
    "#### Purpose of the Convolution\n",
    "- Create spatially equivariant features in the grids\n",
    "- Encourage principles of locality when learning\n",
    "- Temporal equivariance in solution (periodicity encouraged?) - this is not happening currently as the temporal aspect only enters after the convolution\n",
    "\n",
    "TODO: before moving on \n",
    "- Debug the conv neural process (write some tests?)\n",
    "- Implement tests on standard NP \n",
    "- Compare performance  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "TODO \n",
    "- put time into channels not flat layer DONE - also kept in flat layer\n",
    "- play with hyperparams until good outcome \n",
    "\"\"\"\n",
    "\n",
    "conv_neuralprocess = NeuralProcessConv(dimensions)\n",
    "\n",
    "data_loader = DataLoader(meta_data, batch_size=dimensions['batch_size'], shuffle=True)\n",
    "optimizer = torch.optim.Adam(conv_neuralprocess.parameters(), lr=1e-3)\n",
    "conv_np_trainer = NeuralProcessTrainer(\n",
    "    \"cuda\",\n",
    "    conv_neuralprocess,\n",
    "    optimizer,\n",
    "    num_context_range=(dimensions['num_context'], dimensions['num_context']),\n",
    "    num_extra_target_range=(dimensions['num_target'], dimensions['num_target']),\n",
    "    print_freq=200,\n",
    "    grid_size=dimensions['grid_size'],\n",
    ")\n",
    "\n",
    "conv_neuralprocess.training = True\n",
    "conv_np_trainer.train(data_loader, 2)\n",
    "\n",
    "torch.save(neuralprocess.state_dict(), './trainers/conv_neuralprocess_dict')\n",
    "torch.save(conv_np_trainer.epoch_loss_history, './metadata/conv_epoch_loss_history')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L.U.P.I. FOR FLAT NP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuralprocess_pi = NeuralProcess(dimensions)\n",
    "\n",
    "data_loader = DataLoader(meta_data, batch_size=dimensions['batch_size'], shuffle=True)\n",
    "optimizer = torch.optim.Adam(neuralprocess_pi.parameters(), lr=3e-4)\n",
    "np_trainer_pi = NeuralProcessTrainer(\n",
    "    \"cuda\",\n",
    "    neuralprocess_pi,\n",
    "    optimizer,\n",
    "    num_context_range=(dimensions['num_context'], dimensions['num_context']),\n",
    "    num_extra_target_range=(dimensions['num_target'], dimensions['num_target']),\n",
    "    print_freq=200,\n",
    "    is_lupi=True\n",
    ")\n",
    "\n",
    "neuralprocess_pi.training = True\n",
    "np_trainer_pi.train(data_loader, 2)\n",
    "\n",
    "torch.save(neuralprocess.state_dict(), './trainers/neuralprocess_pi_dict')\n",
    "torch.save(np_trainer_pi.epoch_loss_history, './metadata/lupi_epoch_loss_history')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reload Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuralprocess = NeuralProcess(dimensions)\n",
    "neuralprocess.load_state_dict(torch.load('./trainers/neuralprocess_dict'))\n",
    "\n",
    "conv_neuralprocess = NeuralProcess(dimensions)\n",
    "conv_neuralprocess.load_state_dict(torch.load('./trainers/conv_neuralprocess_dict'))\n",
    "\n",
    "neuralprocess_pi = NeuralProcess(dimensions)\n",
    "neuralprocess_pi.load_state_dict(torch.load('./trainers/neuralprocess_pi_dict'))\n",
    "\n",
    "data_loader = DataLoader(meta_data, batch_size=dimensions['batch_size'], shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtain Results from Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(neuralprocess, t_context, y_context, t_target, y_target):\n",
    "    # Switch to test mode\n",
    "    neuralprocess.training = False\n",
    "\n",
    "    # Extract models' distributions over t_target and grid\n",
    "    p_y_pred = neuralprocess(t_context, y_context, t_target)\n",
    "\n",
    "    # flat NP\n",
    "    y_out = p_y_pred.loc.detach().numpy()[0]\n",
    "    var_out = p_y_pred.scale.detach().numpy()[0]\n",
    "\n",
    "    # flat NP reshape\n",
    "    y_out = y_out.reshape((dimensions['max_iter_time'], dimensions['grid_size'], dimensions['grid_size']))\n",
    "    var_out = var_out.reshape((dimensions['max_iter_time'], dimensions['grid_size'], dimensions['grid_size']))\n",
    "\n",
    "    # flat NP result summary\n",
    "    result = [\n",
    "        (y_target[id, :, :], y_out[id, :, :], var_out[id, :, :])\n",
    "        for id in range(y_target.shape[0])\n",
    "    ]\n",
    "\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select which models to test\n",
    "test_convolutional_neural_process = True\n",
    "test_lupi_neural_process = True\n",
    "test_flat_neural_process = True\n",
    "\n",
    "# Extract a batch from data_loader\n",
    "for batch in data_loader:\n",
    "    break\n",
    "\n",
    "# Use batch to create random set of context points\n",
    "t, y, pi = batch\n",
    "t_context, y_context, _, _, _, _ = context_target_split(\n",
    "    t[0:1], y[0:1], pi[0:1], dimensions['num_context'], dimensions['num_target']\n",
    ")\n",
    "\n",
    "# generate random noise between 0 and 1 to feed as context\n",
    "y_noise = 1 * torch.rand(y_context.shape)\n",
    "\n",
    "# Create a set of target points corresponding to in-sample time range\n",
    "t_target = torch.linspace(0, 1, dimensions['max_iter_time'])\n",
    "t_target = t_target.unsqueeze(1).unsqueeze(0)\n",
    "\n",
    "# Extract mean of distribution\n",
    "t_out = t_target.numpy()[0]\n",
    "\n",
    "# target output\n",
    "y_target = y[0:1][0, :, :]\n",
    "y_target = y_target.reshape((dimensions['max_iter_time'], dimensions['grid_size'], dimensions['grid_size']))\n",
    "\n",
    "if test_lupi_neural_process:\n",
    "    lupi_result = test_model(neuralprocess_pi, t_context, y_context, t_target, y_target)\n",
    "    lupi_loss = torch.load('./metadata/lupi_epoch_loss_history')\n",
    "    lupi_result_noise = test_model(conv_neuralprocess, t_context, y_noise, t_target, y_target)\n",
    "    \n",
    "    torch.save(lupi_result, \"./results/lupi_result.pt\")\n",
    "    torch.save(lupi_result_noise, \"./results/lupi_result_noise.pt\")\n",
    "\n",
    "if test_flat_neural_process:\n",
    "    result = test_model(neuralprocess, t_context, y_context, t_target, y_target)\n",
    "    loss = torch.load('./metadata/epoch_loss_history')\n",
    "    result_noise = test_model(neuralprocess, t_context, y_noise, t_target, y_target)\n",
    "\n",
    "    torch.save(result, \"./results/np_result.pt\")\n",
    "    torch.save(loss, \"./results/loss.pt\")\n",
    "    torch.save(result_noise, \"./results/result_noise.pt\")\n",
    "\n",
    "if test_convolutional_neural_process:\n",
    "    conv_result = test_model(conv_neuralprocess, t_context, y_context, t_target, y_target)\n",
    "    conv_loss = torch.load('./metadata/conv_epoch_loss_history')\n",
    "    conv_result_noise = test_model(conv_neuralprocess, t_context, y_noise, t_target, y_target)\n",
    "\n",
    "    torch.save(conv_result, \"./results/conv_result.pt\")\n",
    "    torch.save(conv_loss, \"./results/conv_loss.pt\")\n",
    "    torch.save(conv_result_noise, \"./results/conv_result_noise.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make sure data is in correct format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(result) == dimensions['max_iter_time']\n",
    "assert len(result[0]) == 3\n",
    "assert len(lupi_result) == dimensions['max_iter_time']\n",
    "assert len(lupi_result[0]) == 3\n",
    "assert len(conv_result) == dimensions['max_iter_time']\n",
    "assert len(conv_result[0]) == 3\n",
    "\n",
    "\n",
    "assert result[0][0].shape == (grid_size, grid_size)\n",
    "assert result[0][1].shape == (grid_size, grid_size)\n",
    "assert result[0][2].shape == (grid_size, grid_size)\n",
    "\n",
    "assert result_noise[0][0].shape == (grid_size, grid_size)\n",
    "assert result_noise[0][1].shape == (grid_size, grid_size)\n",
    "assert result_noise[0][2].shape == (grid_size, grid_size)\n",
    "\n",
    "assert lupi_result[0][0].shape == (grid_size, grid_size)\n",
    "assert lupi_result[0][1].shape == (grid_size, grid_size)\n",
    "assert lupi_result[0][2].shape == (grid_size, grid_size)\n",
    "\n",
    "assert lupi_result_noise[0][0].shape == (grid_size, grid_size)\n",
    "assert lupi_result_noise[0][1].shape == (grid_size, grid_size)\n",
    "assert lupi_result_noise[0][2].shape == (grid_size, grid_size)\n",
    "\n",
    "assert conv_result[0][0].shape == (grid_size, grid_size)\n",
    "assert conv_result[0][1].shape == (grid_size, grid_size)\n",
    "assert conv_result[0][2].shape == (grid_size, grid_size)\n",
    "\n",
    "assert conv_result_noise[0][0].shape == (grid_size, grid_size)\n",
    "assert conv_result_noise[0][1].shape == (grid_size, grid_size)\n",
    "assert conv_result_noise[0][2].shape == (grid_size, grid_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metric 1: Mean Accuracy\n",
    "\n",
    "To test how well the model performed, we can calculate the accuracy across the grid using MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = Performance(\n",
    "    result, dimensions)\n",
    "print(\n",
    "    \"FLAT NP: MSE accuracy was {:.8f}, with variance {:.8f}\".format(\n",
    "        performance.mean_accuracy, performance.var_accuracy\n",
    "    )\n",
    ")\n",
    "\n",
    "lupi_performance = Performance(lupi_result, dimensions)\n",
    "print(\n",
    "    \"LUPI NP: MSE accuracy was {:.8f}, with variance {:.8f}\".format(\n",
    "        lupi_performance.mean_accuracy, lupi_performance.var_accuracy\n",
    "    )\n",
    ")\n",
    "\n",
    "conv_performance = Performance(conv_result, dimensions)\n",
    "print(\n",
    "    \"CONVOLUTION: MSE accuracy was {:.8f}, with variance {:.8f}\".format(\n",
    "        conv_performance.mean_accuracy, conv_performance.var_accuracy\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metric 2: Plot Losses as Function of Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(loss=loss, lupi_loss=lupi_loss, conv_loss=conv_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metric 3: Heatmaps for Comparison\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in [0, 10, 50, 99]:\n",
    "    print(\"FLAT NP \\n\")\n",
    "    plotheatmap(result, k, dimensions['max_iter_time'], mean_var_only=False)\n",
    "    # plotheatmap(result_noise, k, dimensions['max_iter_time'], mean_var_only=True)\n",
    "\n",
    "for k in [0, 10, 50, 99]:\n",
    "    print(\"\\n\\n\\n LUPI NP \\n\")\n",
    "    plotheatmap(lupi_result, k, dimensions['max_iter_time'], mean_var_only=False)\n",
    "    # plotheatmap(lupi_result_noise, k, dimensions['max_iter_time'], mean_var_only=True)\n",
    "\n",
    "for k in [0, 10, 50, 99]:\n",
    "    print(\"\\n\\n\\n CONV NP \\n\")\n",
    "    plotheatmap(conv_result, k, dimensions['max_iter_time'], mean_var_only=False)\n",
    "    # plotheatmap(conv_result_noise, k, dimensions['max_iter_time'], mean_var_only=True)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metric 4: Plot Residuals and compare to Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [0, 5, 25, 75]:\n",
    "    plot_residuals(conv_result, i, dimensions['max_iter_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heatmap Animation GIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SINGLE HEATMAP GIF\n",
    "------------------\n",
    "\n",
    "- target = True, variance = False gives the target solution \n",
    "- target = False, variance = True gives the variance in the prediction\n",
    "- target = False, variance = False gives the mean of the prediction\n",
    "\"\"\"\n",
    "\n",
    "def plot_and_save_heatmaps(noise: bool, architecture: str):\n",
    "    ''' \n",
    "    Is there a way to plot this gif without defining a function inside a function?\n",
    "    '''\n",
    "    assert architecture in ['flat', 'LUPI', 'conv']\n",
    "\n",
    "    if noise:\n",
    "        for which_plot in ['Target', 'Mean', 'Variance']:\n",
    "\n",
    "            def animate(k, max_iter_time: int, which_plot: str):\n",
    "                single_hm(conv_result[k], k, dimensions['max_iter_time'], which_plot=which_plot)\n",
    "\n",
    "            anim = FuncAnimation(\n",
    "                plt.figure(),\n",
    "                animate,\n",
    "                interval=1,\n",
    "                frames=dimensions['max_iter_time'],\n",
    "                repeat=False,\n",
    "                fargs=(dimensions['max_iter_time'], which_plot)\n",
    "            )\n",
    "            anim.save(\"./animation_results/noisey_{}_{}.gif\".format(architecture, which_plot))\n",
    "    else:\n",
    "        for which_plot in ['Target', 'Mean', 'Variance']:\n",
    "\n",
    "                def animate(k, max_iter_time: int, which_plot: str):\n",
    "                    single_hm(conv_result[k], k, dimensions['max_iter_time'], which_plot=which_plot)\n",
    "\n",
    "                anim = FuncAnimation(\n",
    "                    plt.figure(),\n",
    "                    animate,\n",
    "                    interval=1,\n",
    "                    frames=dimensions['max_iter_time'],\n",
    "                    repeat=False,\n",
    "                    fargs=(dimensions['max_iter_time'], which_plot)\n",
    "                )\n",
    "                anim.save(\"./animation_results/{}_{}.gif\".format(architecture, which_plot))\n",
    "\n",
    "\n",
    "plot_and_save_heatmaps(noise=False, architecture='conv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix this bug for GIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_out = y_out.reshape((dimensions['max_iter_time'], grid_size, grid_size))\n",
    "var_out = var_out.reshape((dimensions['max_iter_time'], grid_size, grid_size))\n",
    "y_target = y_target.reshape((dimensions['max_iter_time'], grid_size, grid_size))\n",
    "\n",
    "result = [\n",
    "    (y_target[id, :, :], y_out[id, :, :], var_out[id, :, :])\n",
    "    for id in range(y_target.shape[0])\n",
    "]\n",
    "\n",
    "# from heat_diffusion_dataset import Initial_Conditions, Diffusion_Data\n",
    "# import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "\n",
    "def plotheatmap(result_k, k, axes):\n",
    "    # Clear the current plot figure\n",
    "    plt.clf()\n",
    "    ax1, ax2, ax3 = axes\n",
    "\n",
    "    # plt.title(f\"Temperature at t = {k * delta_t:.3f} unit time\")\n",
    "    ax1.set_title(f\"Target at t = {k * 4 * np.pi / dimensions['max_iter_time']:.3f} unit time\")\n",
    "    ax1.set_xlabel(\"x\")\n",
    "    ax1.set_ylabel(\"y\")\n",
    "\n",
    "    ax2.set_title(\n",
    "        f\"Predicted Mean at t = {k * 4 * np.pi / dimensions['max_iter_time']:.3f} unit time\"\n",
    "    )\n",
    "    ax2.set_xlabel(\"x\")\n",
    "    ax2.set_ylabel(\"y\")\n",
    "\n",
    "    ax3.set_title(f\"Predicted Var at t = {k * 4 * np.pi / dimensions['max_iter_time']:.3f} unit time\")\n",
    "    ax3.set_xlabel(\"x\")\n",
    "    ax3.set_ylabel(\"y\")\n",
    "\n",
    "    # This is to plot u_k (u at time-step k)\n",
    "    ax1.pcolormesh(result_k[0], cmap=plt.cm.jet, vmin=-1.1, vmax=1.1)\n",
    "    ax2.pcolormesh(result_k[1], cmap=plt.cm.jet, vmin=-1.1, vmax=1.1)\n",
    "    ax3.pcolormesh(result_k[2], cmap=plt.cm.jet, vmin=-1.1, vmax=1.1)\n",
    "\n",
    "    return fig, axes\n",
    "\n",
    "\n",
    "def animate(k):\n",
    "    plotheatmap(result[k], k)\n",
    "\n",
    "    # if not plot_var and not target:\n",
    "    #     plotheatmap(y_out[k], k, delta_t)\n",
    "    # elif plot_var and not target:\n",
    "    #     plotheatmap(var_out[k], k, delta_t)\n",
    "    # else:\n",
    "    #     plotheatmap(y_target[k], k, delta_t)\n",
    "\n",
    "\n",
    "# plot_var = False\n",
    "# target = False\n",
    "fig, axes = plt.subplots(ncols=3, figsize=(15, 4))\n",
    "\n",
    "anim = animation.FuncAnimation(\n",
    "    fig, animate, interval=1, frames=dimensions['max_iter_time'], repeat=False, fargs=(axes,)\n",
    ")\n",
    "anim.save(\"harmonics_solution.gif\")\n",
    "\n",
    "# plot_var=True\n",
    "# anim = animation.FuncAnimation(plt.figure(), animate, interval=1, frames=max_iter_time, repeat=False, fargs=None)\n",
    "# anim.save(\"harmonics_variance.gif\")\n",
    "\n",
    "# target=True\n",
    "# anim = animation.FuncAnimation(plt.figure(), animate, interval=1, frames=max_iter_time, repeat=False, fargs=None)\n",
    "# anim.save(\"harmonics_target.gif\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
